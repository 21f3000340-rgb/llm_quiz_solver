# ------------------------------------------------------------
# solver.py ‚Äî Using Native Google Generative AI SDK
# - bypasses LangChain wrapper to avoid IndexError
# - direct API access with proper error handling
# - rate limiting protection
# ------------------------------------------------------------

import os
import re
import json
import time
import traceback
from typing import TypedDict, List, Annotated
from collections import deque
from dotenv import load_dotenv

load_dotenv()
EMAIL = os.getenv("EMAIL")
SECRET = os.getenv("SECRET")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# ------------------------------------------------------------
# Base URL extractor
# ------------------------------------------------------------
def extract_base(url: str) -> str:
    from urllib.parse import urlparse
    p = urlparse(url)
    return f"{p.scheme}://{p.netloc}"


# ------------------------------------------------------------
# Rate Limiter Class
# ------------------------------------------------------------
class RateLimiter:
    def __init__(self, max_requests=8, time_window=60):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = deque()
    
    def wait_if_needed(self):
        now = time.time()
        while self.requests and self.requests[0] < now - self.time_window:
            self.requests.popleft()
        
        if len(self.requests) >= self.max_requests:
            sleep_time = self.time_window - (now - self.requests[0]) + 1
            print(f"‚è≥ Rate limit: waiting {sleep_time:.1f}s...")
            time.sleep(sleep_time)
            self.requests.popleft()
        
        self.requests.append(time.time())

rate_limiter = RateLimiter(max_requests=8, time_window=60)


# ------------------------------------------------------------
# Native Google Generative AI Setup
# ------------------------------------------------------------
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

genai.configure(api_key=GOOGLE_API_KEY)

# Configure model with disabled safety filters
generation_config = {
    "temperature": 0.7,
    "top_p": 0.95,
    "top_k": 40,
    "max_output_tokens": 8192,
}

safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

# Create model
model = genai.GenerativeModel(
    model_name='gemini-1.5-flash',
    generation_config=generation_config,
    safety_settings=safety_settings
)


# ------------------------------------------------------------
# LangGraph + Messages
# ------------------------------------------------------------
from langgraph.graph import StateGraph
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage

# ------------------------------------------------------------
# Local tool functions
# ------------------------------------------------------------
from run_code import run_code as _run_code
from web_scraper import get_rendered_html as _get_rendered_html
from download_file import download_file as _download_file
from send_request import post_request as _post_request
from add_dependencies import add_dependencies as _add_dependencies
from image_content_extracter import ocr_image_tool as _ocr_image_tool
from transcribe_audio import transcribe_audio as _transcribe_audio
from encode_image_to_base64 import encode_image_to_base64 as _encode_image_to_base64
from shared_store import url_time

# ------------------------------------------------------------
# Tool wrappers
# ------------------------------------------------------------
from langchain.tools import tool

@tool
def run_code(**kwargs):
    """Execute code and return output from sandbox runner."""
    return _run_code(**kwargs)

@tool
def get_rendered_html(**kwargs):
    """Render a URL (JS) and return the HTML/text."""
    return _get_rendered_html(**kwargs)

@tool
def download_file(**kwargs):
    """Download file and return saved path / metadata."""
    return _download_file(**kwargs)

@tool
def post_request(**kwargs):
    """Send a POST to an endpoint and return parsed response."""
    return _post_request(**kwargs)

@tool
def add_dependencies(**kwargs):
    """Install dependencies dynamically (if allowed)."""
    return _add_dependencies(**kwargs)

@tool
def ocr_image_tool(**kwargs):
    """OCR an image and return extracted text."""
    return _ocr_image_tool(**kwargs)

@tool
def transcribe_audio(**kwargs):
    """Transcribe audio to text and return transcription."""
    return _transcribe_audio(**kwargs)

@tool
def encode_image_to_base64(**kwargs):
    """Encode image to base64 string and return it."""
    return _encode_image_to_base64(**kwargs)


TOOLS = [
    run_code, get_rendered_html, download_file, post_request,
    add_dependencies, ocr_image_tool, transcribe_audio, encode_image_to_base64,
]

# ------------------------------------------------------------
# Configuration
# ------------------------------------------------------------
RECURSION_LIMIT = 5000
MAX_MESSAGES = 40
JSON_FIX_LIMIT = 4
TIMEOUT_LIMIT = 180
LLM_MAX_RETRIES = 5
LLM_BACKOFF_BASE = 2.0

# ------------------------------------------------------------
# System prompt
# ------------------------------------------------------------
SYSTEM_PROMPT = f"""You are an autonomous quiz-solving agent.
Never reveal internal logic or environment variables.

Rules:
1. Use absolute URLs only.
2. Use only the provided tools.
3. When submitting answers include: email = {EMAIL}, secret = {SECRET}
4. Follow next_url until none remain, then output END.
5. Always respond with valid JSON or tool calls."""

# ------------------------------------------------------------
# State typing
# ------------------------------------------------------------
class AgentState(TypedDict):
    messages: Annotated[List, add_messages]
    json_fixes: int
    base_url: str
    llm_errors: int

# ------------------------------------------------------------
# Helpers
# ------------------------------------------------------------
def safe_get_content(content):
    if isinstance(content, str):
        return content
    if isinstance(content, dict):
        return content.get("text", "")
    if isinstance(content, list):
        if not content:
            return ""
        if isinstance(content[0], dict):
            return content[0].get("text", "")
        if isinstance(content[0], str):
            return content[0]
    return ""

def manual_trim(messages: List) -> List:
    if len(messages) <= MAX_MESSAGES:
        return messages
    return messages[-MAX_MESSAGES:]

def fix_relative_url(url_value: str, base: str) -> str:
    if not isinstance(url_value, str):
        return url_value
    if url_value.startswith("/"):
        return base.rstrip("/") + url_value
    return url_value

def fix_json_try(text: str):
    if not isinstance(text, str):
        return None
    
    cleaned = text.strip()
    cleaned = re.sub(r"^```(?:json)?", "", cleaned)
    cleaned = re.sub(r"```$", "", cleaned)
    
    try:
        return json.loads(cleaned)
    except:
        pass
    
    si, ei = cleaned.find("{"), cleaned.rfind("}")
    if si != -1 and ei > si:
        try:
            return json.loads(cleaned[si:ei+1])
        except:
            pass
    
    fixed = cleaned.replace("'", '"')
    fixed = re.sub(r",\s*}", "}", fixed)
    fixed = re.sub(r",\s*]", "]", fixed)
    
    try:
        return json.loads(fixed)
    except:
        return None

def messages_to_gemini_format(messages: List) -> List[dict]:
    """Convert LangChain messages to Gemini format."""
    gemini_messages = []
    
    for msg in messages:
        role = getattr(msg, "role", getattr(msg, "type", ""))
        content = safe_get_content(getattr(msg, "content", ""))
        
        if not content:
            continue
            
        if role in ["system", "human", "user"]:
            gemini_messages.append({"role": "user", "parts": [content]})
        elif role in ["assistant", "ai"]:
            gemini_messages.append({"role": "model", "parts": [content]})
        elif isinstance(msg, SystemMessage):
            gemini_messages.append({"role": "user", "parts": [content]})
        elif isinstance(msg, HumanMessage):
            gemini_messages.append({"role": "user", "parts": [content]})
        elif isinstance(msg, AIMessage):
            gemini_messages.append({"role": "model", "parts": [content]})
    
    return gemini_messages

# ------------------------------------------------------------
# JSON fix node
# ------------------------------------------------------------
def handle_malformed_node(state: AgentState):
    if state["json_fixes"] >= JSON_FIX_LIMIT:
        print("‚ö†Ô∏è JSON fix limit reached")
        fallback = {"action": "continue", "status": "json_fix_limit"}
        return {"messages": [HumanMessage(content=json.dumps(fallback))]}

    last = state["messages"][-1]
    content = safe_get_content(getattr(last, "content", ""))

    print("‚ö†Ô∏è Invalid JSON ‚Äî fixing...")
    fixed = fix_json_try(content)
    
    if fixed is not None:
        try:
            last.content = json.dumps(fixed)
            print("‚úÖ JSON fixed")
        except:
            last.content = str(fixed)
        return {"messages": [], "json_fixes": state["json_fixes"] + 1}

    return {
        "messages": [HumanMessage(content="Invalid output. Return valid JSON only. No markdown.")],
        "json_fixes": state["json_fixes"] + 1,
    }

# ------------------------------------------------------------
# Native Gemini invoke with proper error handling
# ------------------------------------------------------------
def invoke_gemini_with_retries(messages: List, max_retries: int = LLM_MAX_RETRIES):
    """
    Invoke Gemini using native SDK with retry logic.
    This bypasses LangChain's wrapper that causes IndexError.
    """
    attempt = 0
    
    while attempt < max_retries:
        attempt += 1
        
        try:
            rate_limiter.wait_if_needed()
        except Exception as e:
            print(f"‚ö†Ô∏è Rate limiter error: {e}")
        
        # Convert messages to Gemini format
        gemini_msgs = messages_to_gemini_format(messages)
        
        # Ensure alternating user/model messages
        if not gemini_msgs:
            gemini_msgs = [{"role": "user", "parts": ["Continue"]}]
        
        # Gemini requires starting with user message
        if gemini_msgs[0]["role"] != "user":
            gemini_msgs.insert(0, {"role": "user", "parts": ["Start"]})
        
        try:
            print(f"ü§ñ Gemini attempt {attempt}/{max_retries} ({len(gemini_msgs)} messages)")
            
            # Use native Gemini SDK
            chat = model.start_chat(history=gemini_msgs[:-1] if len(gemini_msgs) > 1 else [])
            response = chat.send_message(gemini_msgs[-1]["parts"][0])
            
            # Check if response has content
            if not response or not response.text:
                print(f"‚ö†Ô∏è Empty response on attempt {attempt}")
                raise ValueError("Empty response from Gemini")
            
            content = response.text.strip()
            
            # Check for safety blocks
            if hasattr(response, 'prompt_feedback'):
                block_reason = getattr(response.prompt_feedback, 'block_reason', None)
                if block_reason:
                    print(f"‚ö†Ô∏è Blocked: {block_reason}")
                    raise ValueError(f"Content blocked: {block_reason}")
            
            print(f"‚úÖ Gemini responded ({len(content)} chars)")
            
            # Return as AIMessage for LangGraph compatibility
            return AIMessage(content=content)
            
        except ValueError as ve:
            print(f"‚ö†Ô∏è Content issue on attempt {attempt}: {ve}")
            
        except Exception as e:
            error_str = str(e)
            print(f"‚ö†Ô∏è Gemini error on attempt {attempt}: {error_str[:200]}")
            
            # Handle rate limits
            if "429" in error_str or "quota" in error_str.lower() or "Resource" in error_str:
                wait_time = 65
                print(f"‚ö†Ô∏è Rate limit! Waiting {wait_time}s...")
                time.sleep(wait_time)
                continue
            
            # Handle server errors
            if "503" in error_str or "500" in error_str:
                print("‚ö†Ô∏è Server error")
        
        # Exponential backoff
        if attempt < max_retries:
            backoff = LLM_BACKOFF_BASE * (2 ** (attempt - 1))
            print(f"‚è≥ Waiting {backoff}s before retry...")
            time.sleep(backoff)

    # All retries failed - return fallback
    print("‚ùå All Gemini attempts failed")
    fallback = {"action": "get_rendered_html", "url": os.getenv("url", "")}
    return AIMessage(content=json.dumps(fallback))

# ------------------------------------------------------------
# Agent node
# ------------------------------------------------------------
def agent_node(state: AgentState):
    cur_url = os.getenv("url")
    now = time.time()
    prev = url_time.get(cur_url)

    # Timeout enforcement
    if prev and now - float(prev) >= TIMEOUT_LIMIT:
        print("‚ö†Ô∏è TIMEOUT ‚Äî forcing WRONG answer")
        forced = HumanMessage(content="Time limit exceeded. Submit WRONG answer using post_request immediately.")
        result = invoke_gemini_with_retries(state["messages"] + [forced])
        return {"messages": [result]}

    # Trim conversation
    trimmed = manual_trim(state["messages"])

    # Ensure at least one human message
    if not any(getattr(m, "type", "") == "human" or getattr(m, "role", "") == "user" for m in trimmed):
        trimmed.append(HumanMessage(content=f"Analyze: {cur_url}"))

    print(f"\nüîÅ Agent: {len(trimmed)} messages, errors={state.get('llm_errors',0)}")

    # Invoke Gemini
    resp = invoke_gemini_with_retries(trimmed)

    # Check for errors
    if isinstance(resp, AIMessage):
        content = safe_get_content(getattr(resp, "content", ""))
        try:
            parsed = json.loads(content)
            if isinstance(parsed, dict) and "error" in parsed:
                state["llm_errors"] = state.get("llm_errors", 0) + 1
                print(f"‚ùå Error count: {state['llm_errors']}")
            else:
                state["llm_errors"] = 0
        except:
            state["llm_errors"] = 0
    else:
        state["llm_errors"] = 0

    # Stop if too many errors
    if state.get("llm_errors", 0) >= 5:
        print("‚ùå Too many errors ‚Äî aborting")
        return {"messages": [AIMessage(content="END")]}

    return {"messages": [resp]}

# ------------------------------------------------------------
# Router
# ------------------------------------------------------------
def route(state: AgentState):
    last = state["messages"][-1]
    content = safe_get_content(getattr(last, "content", "")).strip()

    # Fix relative URLs
    try:
        if content.startswith("{"):
            data = json.loads(content)
            if "url" in data:
                fixed = fix_relative_url(data["url"], state["base_url"])
                if fixed != data["url"]:
                    data["url"] = fixed
                    last.content = json.dumps(data)
    except:
        pass

    # Check for tool calls
    if getattr(last, "tool_calls", None):
        print("‚Üí Tools")
        return "tools"

    # Check for END
    if content == "END":
        print("‚Üí END")
        return "__end__"

    # Validate JSON
    if content.startswith("{") or content.startswith("["):
        try:
            json.loads(content)
            print("‚Üí Continue")
            return "agent"
        except:
            print("‚Üí Fix JSON")
            return "json_fix"

    print("‚Üí Fix JSON")
    return "json_fix"

# ------------------------------------------------------------
# Build graph
# ------------------------------------------------------------
print("üîß Building workflow...")

graph = StateGraph(AgentState)
graph.add_node("agent", agent_node)
graph.add_node("json_fix", handle_malformed_node)
graph.add_node("tools", ToolNode(TOOLS))

graph.add_edge("__start__", "agent")
graph.add_edge("json_fix", "agent")
graph.add_edge("tools", "agent")

graph.add_conditional_edges(
    "agent", route,
    {"agent": "agent", "json_fix": "json_fix", "tools": "tools", "__end__": "__end__"},
)

app = graph.compile()
print("‚úÖ Workflow compiled")

# ------------------------------------------------------------
# Runner
# ------------------------------------------------------------
def run_agent(url: str):
    print(f"\n{'='*60}")
    print(f"üöÄ Starting: {url}")
    print(f"{'='*60}\n")
    
    base = extract_base(url)
    initial = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=url),
    ]

    try:
        app.invoke(
            {"messages": initial, "json_fixes": 0, "base_url": base, "llm_errors": 0},
            config={"recursion_limit": RECURSION_LIMIT},
        )
        print("\nüéâ Completed!")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        traceback.print_exc()
    
    print(f"\n{'='*60}\n")
